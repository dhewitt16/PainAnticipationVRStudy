{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8c62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats, ndimage\n",
    "from scipy.signal import butter, lfilter, medfilt\n",
    "\n",
    "#Change these for the data\n",
    "\n",
    "trialLength = 3300\n",
    "baselineLength = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742020fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_column_wise_average(df):\n",
    "    columns_to_average = df.columns[5:3305].values\n",
    "    return df[columns_to_average].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26160dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants_data = pd.DataFrame()\n",
    "all_ext_participants_data = pd.DataFrame()\n",
    "path = \"/Users/dhewitt/Data/pps/\"\n",
    "\n",
    "def process_participant(participantID):\n",
    "    global all_participants_data\n",
    "    global all_ext_participants_data\n",
    "    \n",
    "    PKL_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_dataframelist.pkl\")\n",
    "    CSV_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_Extracted_1504.csv\")\n",
    "    OUT_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_GazeDirectionProcessed.csv\")\n",
    "    AV_OUT_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_GazeDirectionProcessed_noAv.csv\")\n",
    "    EXT_AV_OUT_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_GazeDirectionProcessedExtAv.csv\")\n",
    "\n",
    "    data2load = pd.read_csv(CSV_FILE_NAME) \n",
    "    dataframe_list = pd.read_pickle(PKL_FILE_NAME)\n",
    "\n",
    "    #Interpolating the df due to changes in sampling rate for collecting data between trials.\n",
    "    #Normalising the timestamp from the first timepoint.\n",
    "\n",
    "    def extract_and_interpolate_gaze_direction(df, trial_number):\n",
    "        gaze_direction = df['gaze_direction'].apply(pd.Series)  # Expand tuple into separate columns\n",
    "        interpolated_gaze_direction = pd.DataFrame({\n",
    "        'gaze_direction_x': np.interp(np.arange(baselineLength + trialLength),\n",
    "                                      df['timestamp'] - df['timestamp'].iloc[0],\n",
    "                                      gaze_direction[0]),\n",
    "        'gaze_direction_y': np.interp(np.arange(baselineLength + trialLength),\n",
    "                                      df['timestamp'] - df['timestamp'].iloc[0],\n",
    "                                      gaze_direction[1]),\n",
    "        'gaze_direction_z': np.interp(np.arange(baselineLength + trialLength),\n",
    "                                      df['timestamp'] - df['timestamp'].iloc[0],\n",
    "                                      gaze_direction[2]),\n",
    "        'trial': trial_number\n",
    "        })\n",
    "        return interpolated_gaze_direction\n",
    "\n",
    "    # Apply the function to each DataFrame in the list\n",
    "    gaze_direction_data = [extract_and_interpolate_gaze_direction(df, trial_number)\n",
    "                       for trial_number, df in enumerate(dataframe_list, start=1)]\n",
    "\n",
    "    # Concatenate the individual DataFrames into one large DataFrame\n",
    "    GazeDirection = pd.concat(gaze_direction_data, ignore_index=False)\n",
    "    GazeDirection = GazeDirection.reset_index().rename(columns={'index':'timepoint'})\n",
    "    GazeDirection = GazeDirection.drop(['gaze_direction_y','gaze_direction_z'],axis=1)\n",
    "    GazeDirection = GazeDirection.pivot(index='trial', columns='timepoint', values='gaze_direction_x')\n",
    "    GazeDirection = GazeDirection.reset_index(drop=True)\n",
    "    \n",
    "    df1 = pd.DataFrame(GazeDirection)\n",
    "    total_mean = df1.stack().mean()\n",
    "    total_std = df1.stack().std()\n",
    "    data_columns = df1.columns\n",
    "\n",
    "    # Create an empty list to store dictionaries of blink indices\n",
    "    blink_indices_list = []\n",
    "\n",
    "    # Create a function to calculate blink counts and return indices\n",
    "    def calculate_blink_indices(row):\n",
    "        trial_data = row[data_columns]\n",
    "        trial_mean = trial_data.mean()\n",
    "        trial_std = trial_data.std()\n",
    "        blink_data_points = np.abs(trial_data - trial_mean) > 2 * trial_std\n",
    "        #blink_data_points = np.abs(trial_data - total_mean) > 3 * total_std\n",
    "        column_indices = np.where(blink_data_points)[0]\n",
    "        blink_indices = [{\"Row Index\": row.name, \"Column Index\": data_columns[col_idx]} for col_idx in column_indices]\n",
    "        blink_indices_list.extend(blink_indices)\n",
    "\n",
    "    df1.apply(calculate_blink_indices, axis=1)\n",
    "    blink_indices_df = pd.DataFrame(blink_indices_list)\n",
    "\n",
    "    sorted_df = blink_indices_df.sort_values(by=['Row Index', 'Column Index'])\n",
    "    diff = sorted_df['Column Index'].diff()\n",
    "    group = (diff != 1).cumsum()\n",
    "    grouped_data = sorted_df.groupby(['Row Index', group], as_index=False)['Column Index'].agg(['first', 'last'])\n",
    "    grouped_data.columns = ['Start Column Index', 'End Column Index']\n",
    "    grouped_data['Total Dur'] = grouped_data['End Column Index']-grouped_data['Start Column Index']\n",
    "\n",
    "    threshold = 150;\n",
    "\n",
    "    grouped_data = grouped_data[grouped_data['Total Dur']>=threshold].reset_index().drop(columns='Column Index')\n",
    "    \n",
    "    ###Getting rid of any trials with too much extreme data - 25% of all trials\n",
    "    \n",
    "    interpolated_df1 = df1.copy()\n",
    "\n",
    "    # Iterate over the rows and matching columns in filtered_grouped_data\n",
    "    for _, row_data in grouped_data.iterrows():\n",
    "        row_index = row_data['Row Index']\n",
    "        start_col_index = row_data['Start Column Index']\n",
    "        end_col_index = row_data['End Column Index']\n",
    "\n",
    "        # Ensure the row index is present in the DataFrame\n",
    "        if row_index in interpolated_df1.index:\n",
    "        # Replace matching data with NaN for interpolation\n",
    "            interpolated_df1.iloc[row_index, start_col_index:end_col_index + 1] = np.nan\n",
    "\n",
    "    # Interpolate the missing values using linear interpolation\n",
    "    interpolated_df1 = interpolated_df1.interpolate(method='linear', axis=1, limit_direction='both')\n",
    "    \n",
    "    # Filter out 'Row Index' in grouped_data with 'Total Dur' greater than 825\n",
    "    rows_to_remove = grouped_data[grouped_data['Total Dur'] > 1025]['Row Index']\n",
    "\n",
    "    # Drop rows in interpolated_df2 corresponding to the selected 'Row Index'\n",
    "    df1 = interpolated_df1.drop(rows_to_remove, errors='ignore')\n",
    "\n",
    "    # Reset the index of the DataFrame after dropping rows\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Filtering would go here if needed\n",
    "    \n",
    "    filtered_df1 = medfilt(df1, 5)\n",
    "    df1 = pd.DataFrame(filtered_df1, columns=df1.columns)\n",
    "    \n",
    "    #Baseline correcting -- currently turned off\n",
    "\n",
    "    baselineStart=250\n",
    "    baselineEnd=750\n",
    "\n",
    "    #baselineAv = df1.iloc[:,baselineStart:baselineEnd].mean(axis=1)\n",
    "    #df1 = df1.apply(lambda x: (x - baselineAv)*1000)\n",
    "    df1 = df1.apply(lambda x: x * 1000)\n",
    "    \n",
    "    df1.drop(df1.iloc[:,0:baselineLength], inplace=True, axis=1) #getting rid of the baseline\n",
    "    columns = dict(map(reversed, enumerate(df1.columns))) #resetting the index\n",
    "    df1 = df1.rename(columns=columns)\n",
    "\n",
    "    # Now getting the condition data\n",
    "\n",
    "    df = pd.DataFrame(data2load)\n",
    "    newdf = pd.concat([df,df1],axis=1)\n",
    "\n",
    "        # Creating averages for each column (i.e., over all trials for each condition)\n",
    "\n",
    "    condMiddle = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 1)]\n",
    "    Middle_cond_average1 = calculate_column_wise_average(condMiddle)\n",
    "\n",
    "    condLeft = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 1)]\n",
    "    Left_cond_average1 = calculate_column_wise_average(condLeft)\n",
    "\n",
    "    condRight = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 1)]\n",
    "    Right_cond_average1 = calculate_column_wise_average(condRight) \n",
    "\n",
    "    extMiddle = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 2)]\n",
    "    Middle_ext_average1 = calculate_column_wise_average(extMiddle)\n",
    "\n",
    "    extLeft = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 2)]\n",
    "    Left_ext_average1 = calculate_column_wise_average(extLeft)\n",
    "\n",
    "    extRight = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 2)]\n",
    "    Right_ext_average1 = calculate_column_wise_average(extRight)    \n",
    "    \n",
    "    #### second run\n",
    "    condMiddle = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 3)]\n",
    "    Middle_cond_average2 = calculate_column_wise_average(condMiddle)\n",
    "\n",
    "    condLeft = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 3)]\n",
    "    Left_cond_average2 = calculate_column_wise_average(condLeft)\n",
    "\n",
    "    condRight = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 3)]\n",
    "    Right_cond_average2 = calculate_column_wise_average(condRight) \n",
    "\n",
    "    extMiddle = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 4)]\n",
    "    Middle_ext_average2 = calculate_column_wise_average(extMiddle)\n",
    "\n",
    "    extLeft = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 4)]\n",
    "    Left_ext_average2 = calculate_column_wise_average(extLeft)\n",
    "\n",
    "    extRight = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 4)]\n",
    "    Right_ext_average2 = calculate_column_wise_average(extRight)   \n",
    "\n",
    "    # Saving single subject data for pain-cue-block comparisons\n",
    "\n",
    "    frames = [Middle_cond_average1, Left_cond_average1, Right_cond_average1, Middle_ext_average1, Left_ext_average1, Right_ext_average1,\n",
    "             Middle_cond_average2, Left_cond_average2, Right_cond_average2, Middle_ext_average2, Left_ext_average2, Right_ext_average2]\n",
    "    avscore = pd.concat(frames, axis=1, keys = [\"CM1\", \"CL1\", \"CR1\", \"EM1\", \"EL1\", \"ER1\",\n",
    "                                               \"CM2\", \"CL2\", \"CR2\", \"EM2\", \"EL2\", \"ER2\"])\n",
    "    avscore.to_csv(AV_OUT_FILE_NAME)\n",
    "    \n",
    "    # Saving single subject data for extinction-tonic pain comparisons\n",
    "    \n",
    "    extMiddle1 = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 2)]\n",
    "    Middle_ext_average1 = calculate_column_wise_average(extMiddle1)\n",
    "\n",
    "    extLeft1 = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 2)]\n",
    "    Left_ext_average1 = calculate_column_wise_average(extLeft1)\n",
    "\n",
    "    extRight1 = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 2)]\n",
    "    Right_ext_average1 = calculate_column_wise_average(extRight1)    \n",
    "    \n",
    "    extMiddle2 = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 4)]\n",
    "    Middle_ext_average2 = calculate_column_wise_average(extMiddle2)\n",
    "\n",
    "    extLeft2 = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 4)]\n",
    "    Left_ext_average2 = calculate_column_wise_average(extLeft2)\n",
    "\n",
    "    extRight2 = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 4)]\n",
    "    Right_ext_average2 = calculate_column_wise_average(extRight2)    \n",
    "    \n",
    "    frames = [Middle_ext_average1, Left_ext_average1, Right_ext_average1, Middle_ext_average2, Left_ext_average2, Right_ext_average2]\n",
    "    avscore = pd.concat(frames, axis=1, keys = [\"EM1\", \"EL1\", \"ER1\", \"EM2\", \"EL2\", \"ER2\"])\n",
    "    avscore.to_csv(EXT_AV_OUT_FILE_NAME)\n",
    "    \n",
    "    # Read the CSV file for the current participant\n",
    "    participant_data = pd.read_csv(AV_OUT_FILE_NAME)\n",
    "    participant_data[\"Participant\"] = participantID\n",
    "    all_participants_data = pd.concat([all_participants_data, participant_data], ignore_index=True)\n",
    "    \n",
    "    ext_participant_data = pd.read_csv(EXT_AV_OUT_FILE_NAME)\n",
    "    ext_participant_data[\"Participant\"] = participantID\n",
    "    all_ext_participants_data = pd.concat([all_ext_participants_data, ext_participant_data], ignore_index=True)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb7d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of participant IDs\n",
    "participant_ids = [\"02\", \"03\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"19\", \"20\", \"21\", \"22\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\",\"30\"] \n",
    "#participant_ids = [\"02\"]\n",
    "\n",
    "# Loop over each participant\n",
    "for participantID in participant_ids:\n",
    "    # ... (The existing code for processing a single participant will go here)\n",
    "    process_participant(participantID)\n",
    "\n",
    "# Concatenate all participant data into a single DataFrame\n",
    "all_participants_data.to_csv(\"/Users/dhewitt/Data/pps/Exports/Combined_GazeDirection_noav_1504.csv\", index=False)\n",
    "all_ext_participants_data.to_csv(\"/Users/dhewitt/Data/pps/Exports/Combined_GazeDirectionExt_1504.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958ebcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56279615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
