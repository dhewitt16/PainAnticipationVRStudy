{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8c62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats, ndimage\n",
    "from scipy.signal import butter, lfilter, medfilt\n",
    "\n",
    "#Change these for the data\n",
    "\n",
    "trialLength = 3000\n",
    "baselineLength = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742020fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_column_wise_average(df):\n",
    "    columns_to_average = df.columns[5:3305].values\n",
    "    return df[columns_to_average].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26160dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants_data = pd.DataFrame()\n",
    "all_ext_participants_data = pd.DataFrame()\n",
    "\n",
    "def process_participant(participantID):\n",
    "    global all_participants_data\n",
    "    global all_ext_participants_data\n",
    "    path = \"/Users/dhewitt/Data/pps/\"\n",
    "    \n",
    "    PKL_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_dataframelist.pkl\")\n",
    "    CSV_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_Extracted_1504.csv\")\n",
    "    OUT_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_PupilDiameterProcessed_1904.csv\")\n",
    "    STT_OUT_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID  + \"_PupilDiameterProcessed_ST.csv\")\n",
    "    AV_OUT_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID + \"_PupilDiameterProcessed_noAv_1904.csv\")\n",
    "    EXT_AV_OUT_FILE_NAME = os.path.join(path + \"P\" + participantID + \"/P\" + participantID + \"_PupilDiameterProcessed_AvExt_1904.csv\")\n",
    "\n",
    "    data2load = pd.read_csv(CSV_FILE_NAME) \n",
    "    dataframe_list = pd.read_pickle(PKL_FILE_NAME)\n",
    "\n",
    "    #Normalising the timestamp from the first timepoint.\n",
    "    #Just getting the 'in trial' bit from trial start/cue appearing to the pain stimulus onset.\n",
    "\n",
    "    Right_extracted_pupil_sizes = list(map(lambda df: np.interp(np.arange(baselineLength+trialLength), df['timestamp'] - df['timestamp'][0], \n",
    "                    df['right_pupil_diameter_mm'].to_numpy()), dataframe_list))\n",
    "\n",
    "    Left_extracted_pupil_sizes = list(map(lambda df: np.interp(np.arange(baselineLength+trialLength), df['timestamp'] - df['timestamp'][0], \n",
    "                    df['left_pupil_diameter_mm'].to_numpy()), dataframe_list))\n",
    "\n",
    "    # Now getting the blinks and interpolating them - starting with the right pupil\n",
    "    df1 = pd.DataFrame(Right_extracted_pupil_sizes)\n",
    "    total_mean = df1.stack().mean()\n",
    "    total_std = df1.stack().std()\n",
    "\n",
    "    # Get blinks\n",
    "    data_columns = df1.columns\n",
    "    blink_indices_list = []\n",
    "    \n",
    "    def calculate_blink_indices(row):\n",
    "        trial_data = row[data_columns]     # Extract the data points for the current trial\n",
    "        trial_mean = trial_data.mean()\n",
    "        trial_std = trial_data.std()\n",
    "        blink_data_points = np.abs(trial_data - trial_mean) > 2 * trial_std     # Identify data points more than 2 standard deviations from the mean\n",
    "        #blink_data_points = np.abs(trial_data - total_mean) > 3 * total_std\n",
    "        column_indices = np.where(blink_data_points)[0] # Get the column indices where blink_data_points are True\n",
    "        blink_indices = [{\"Row Index\": row.name, \"Column Index\": data_columns[col_idx]} for col_idx in column_indices] # Create a list of dictionaries with row and column indices\n",
    "        blink_indices_list.extend(blink_indices)\n",
    "    \n",
    "    df1.apply(calculate_blink_indices, axis=1)\n",
    "    blink_indices_df = pd.DataFrame(blink_indices_list)\n",
    "\n",
    "    sorted_df = blink_indices_df.sort_values(by=['Row Index', 'Column Index'])\n",
    "    diff = sorted_df['Column Index'].diff()\n",
    "    # Identify gaps in column indices and create a new group when gaps are not equal to 1\n",
    "    group = (diff != 1).cumsum()\n",
    "    # Group the data by 'Row Index' and the calculated 'group'\n",
    "    grouped_data = sorted_df.groupby(['Row Index', group], as_index=False)['Column Index'].agg(['first', 'last'])\n",
    "\n",
    "    # Rename the columns for clarity\n",
    "    grouped_data.columns = ['Start Column Index', 'End Column Index']\n",
    "    grouped_data['Total Dur'] = grouped_data['End Column Index']-grouped_data['Start Column Index']\n",
    "\n",
    "    threshold = 150;\n",
    "    grouped_data = grouped_data[grouped_data['Total Dur']>=threshold].reset_index().drop(columns='Column Index')\n",
    "\n",
    "    interpolated_df1 = df1.copy()\n",
    "    # Iterate over the rows and matching columns in filtered_grouped_data\n",
    "    for _, row_data in grouped_data.iterrows():\n",
    "        row_index = row_data['Row Index']\n",
    "        start_col_index = row_data['Start Column Index']\n",
    "        end_col_index = row_data['End Column Index']\n",
    "\n",
    "        # Ensure the row index is present in the DataFrame\n",
    "        if row_index in interpolated_df1.index:\n",
    "            # Replace matching data with NaN for interpolation\n",
    "            #print(row_index, start_col_index, end_col_index)\n",
    "            interpolated_df1.iloc[row_index, start_col_index:end_col_index + 1] = np.nan\n",
    "\n",
    "    # Interpolate the missing values using linear interpolation\n",
    "    interpolated_df1 = interpolated_df1.interpolate(method='linear', axis=1, limit_direction='both')\n",
    "\n",
    "    #matching_df.to_csv('/Users/dhewitt/Data/pps/P03/P03_matching.csv')\n",
    "    #interpolated_df1.to_csv('/Users/dhewitt/Data/pps/P03/P03_interpdf_nan00.csv')\n",
    "\n",
    "    ###Getting rid of any trials with too many interpolations - 25% of all trials\n",
    "\n",
    "    # Filter out 'Row Index' in grouped_data with 'Total Dur' greater than 825\n",
    "    rows_to_remove = grouped_data[grouped_data['Total Dur'] > 1025]['Row Index']\n",
    "\n",
    "    # Drop rows in interpolated_df2 corresponding to the selected 'Row Index'\n",
    "    interpolated_df1 = interpolated_df1.drop(rows_to_remove, errors='ignore')\n",
    "    interpolated_df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Now the left pupil\n",
    "\n",
    "    df2 = pd.DataFrame(Left_extracted_pupil_sizes)\n",
    "    total_mean = df2.stack().mean()\n",
    "    total_std = df2.stack().std()\n",
    "\n",
    "    # Create an empty list to store dictionaries of blink indices\n",
    "    data_columns = df2.columns\n",
    "    blink_indices_list = []\n",
    "    \n",
    "    def calculate_blink_indices(row):\n",
    "        trial_data = row[data_columns]     # Extract the data points for the current trial\n",
    "        trial_mean = trial_data.mean()\n",
    "        trial_std = trial_data.std()\n",
    "        blink_data_points = np.abs(trial_data - trial_mean) > 2 * trial_std     # Identify data points more than 2 standard deviations from the mean\n",
    "        #blink_data_points = np.abs(trial_data - total_mean) > 3 * total_std\n",
    "        column_indices = np.where(blink_data_points)[0] # Get the column indices where blink_data_points are True\n",
    "        blink_indices = [{\"Row Index\": row.name, \"Column Index\": data_columns[col_idx]} for col_idx in column_indices] # Create a list of dictionaries with row and column indices\n",
    "        blink_indices_list.extend(blink_indices)\n",
    "    \n",
    "    \n",
    "    df2.apply(calculate_blink_indices, axis=1)\n",
    "    blink_indices_df = pd.DataFrame(blink_indices_list)\n",
    "\n",
    "    sorted_df = blink_indices_df.sort_values(by=['Row Index', 'Column Index'])\n",
    "    diff = sorted_df['Column Index'].diff()\n",
    "    # Identify gaps in column indices and create a new group when gaps are not equal to 1\n",
    "    group = (diff != 1).cumsum()\n",
    "    # Group the data by 'Row Index' and the calculated 'group'\n",
    "    grouped_data = sorted_df.groupby(['Row Index', group], as_index=False)['Column Index'].agg(['first', 'last'])\n",
    "    grouped_data.columns = ['Start Column Index', 'End Column Index']\n",
    "    grouped_data['Total Dur'] = grouped_data['End Column Index']-grouped_data['Start Column Index']\n",
    "\n",
    "    threshold = 150;\n",
    "\n",
    "    grouped_data = grouped_data[grouped_data['Total Dur']>=threshold].reset_index().drop(columns='Column Index')\n",
    "\n",
    "    interpolated_df2 = df2.copy()\n",
    "    # Iterate over the rows and matching columns in grouped_data\n",
    "    for _, row_data in grouped_data.iterrows():\n",
    "        row_index = row_data['Row Index']\n",
    "        start_col_index = row_data['Start Column Index']\n",
    "        end_col_index = row_data['End Column Index']\n",
    "\n",
    "        # Ensure the row index is present in the DataFrame\n",
    "        if row_index in interpolated_df2.index:\n",
    "            #print(row_index, start_col_index, end_col_index)\n",
    "            # Replace matching data with NaN for interpolation\n",
    "            interpolated_df2.iloc[row_index, start_col_index:end_col_index + 1] = np.nan\n",
    "\n",
    "    # Interpolate the missing values using linear interpolation\n",
    "    interpolated_df2 = interpolated_df2.interpolate(method='linear', axis=1, limit_direction='both')\n",
    "\n",
    "    #df2.to_csv('/Users/dhewitt/Data/pps/P03/P03_matching.csv')\n",
    "    #interpolated_df2.to_csv('/Users/dhewitt/Data/pps/P03/P03_interpdf2_monday.csv')\n",
    "\n",
    "    ###Getting rid of any trials with too many interpolations - 25% of all trials\n",
    "\n",
    "    # Filter out 'Row Index' in grouped_data with 'Total Dur' greater than 825\n",
    "    rows_to_remove = grouped_data[grouped_data['Total Dur'] > 1025]['Row Index']\n",
    "    interpolated_df2 = interpolated_df2.drop(rows_to_remove, errors='ignore')\n",
    "    interpolated_df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ##Filtering\n",
    "\n",
    "    ## Apply the filter to interpolated_df1 and interpolated_df2\n",
    "    #filtered_interpolated_df1 = medfilt(interpolated_df1, 3)\n",
    "    #filtered_interpolated_df2 =  medfilt(interpolated_df2, 3)\n",
    "    filtered_interpolated_df1 = interpolated_df1\n",
    "    filtered_interpolated_df2 =  interpolated_df2\n",
    "\n",
    "    #filtered_interpolated_df1 = ndimage.median_filter(interpolated_df1, size=11)\n",
    "    #filtered_interpolated_df2 =  ndimage.median_filter(interpolated_df2, size=11)\n",
    "\n",
    "    ## Create DataFrames from the filtered arrays\n",
    "    interpolated_df1 = pd.DataFrame(filtered_interpolated_df1, columns=interpolated_df1.columns)\n",
    "    interpolated_df2 = pd.DataFrame(filtered_interpolated_df2, columns=interpolated_df2.columns)\n",
    "\n",
    "    # Baseline correcting\n",
    "    baselineStart=250\n",
    "    baselineEnd=750\n",
    "\n",
    "    baselineAv = interpolated_df1.iloc[:,baselineStart:baselineEnd].mean(axis=1)\n",
    "    interpolated_df1 = interpolated_df1.apply(lambda x: x - baselineAv)\n",
    "\n",
    "    baselineAv = interpolated_df2.iloc[:,baselineStart:baselineEnd].mean(axis=1)\n",
    "    interpolated_df2 = interpolated_df2.apply(lambda x: x - baselineAv)\n",
    "\n",
    "    # Concatenating left and right eyes\n",
    "\n",
    "    df1 = pd.DataFrame(interpolated_df1) #right pupil sizes after blink interpolation\n",
    "    df2 = pd.DataFrame(interpolated_df2) #left pupil sizes after blink interpolation\n",
    "\n",
    "    botheyesdf = (df1+df2)/2\n",
    "\n",
    "    botheyesdf.drop(botheyesdf.iloc[:,0:baselineLength], inplace=True, axis=1) #getting rid of the baseline\n",
    "    columns = dict(map(reversed, enumerate(botheyesdf.columns))) #resetting the index\n",
    "    botheyesdf = botheyesdf.rename(columns=columns)\n",
    "\n",
    "    df = pd.DataFrame(data2load)\n",
    "    newdf = pd.concat([df,botheyesdf],axis=1)\n",
    "    \n",
    "    newdf.to_csv(STT_OUT_FILE_NAME, index=False)\n",
    "\n",
    "    # Creating averages for each column (i.e., over all trials for each condition)\n",
    "\n",
    "    condMiddle = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 1)]\n",
    "    Middle_cond_average1 = calculate_column_wise_average(condMiddle)\n",
    "\n",
    "    condLeft = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 1)]\n",
    "    Left_cond_average1 = calculate_column_wise_average(condLeft)\n",
    "\n",
    "    condRight = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 1)]\n",
    "    Right_cond_average1 = calculate_column_wise_average(condRight) \n",
    "\n",
    "    extMiddle = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 2)]\n",
    "    Middle_ext_average1 = calculate_column_wise_average(extMiddle)\n",
    "\n",
    "    extLeft = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 2)]\n",
    "    Left_ext_average1 = calculate_column_wise_average(extLeft)\n",
    "\n",
    "    extRight = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 2)]\n",
    "    Right_ext_average1 = calculate_column_wise_average(extRight)    \n",
    "    \n",
    "    #### second run\n",
    "    condMiddle = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 3)]\n",
    "    Middle_cond_average2 = calculate_column_wise_average(condMiddle)\n",
    "\n",
    "    condLeft = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 3)]\n",
    "    Left_cond_average2 = calculate_column_wise_average(condLeft)\n",
    "\n",
    "    condRight = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 3)]\n",
    "    Right_cond_average2 = calculate_column_wise_average(condRight) \n",
    "\n",
    "    extMiddle = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 4)]\n",
    "    Middle_ext_average2 = calculate_column_wise_average(extMiddle)\n",
    "\n",
    "    extLeft = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 4)]\n",
    "    Left_ext_average2 = calculate_column_wise_average(extLeft)\n",
    "\n",
    "    extRight = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 4)]\n",
    "    Right_ext_average2 = calculate_column_wise_average(extRight)   \n",
    "\n",
    "    # Saving single subject data for pain-cue-block comparisons\n",
    "\n",
    "    frames = [Middle_cond_average1, Left_cond_average1, Right_cond_average1, Middle_ext_average1, Left_ext_average1, Right_ext_average1,\n",
    "             Middle_cond_average2, Left_cond_average2, Right_cond_average2, Middle_ext_average2, Left_ext_average2, Right_ext_average2]\n",
    "    avscore = pd.concat(frames, axis=1, keys = [\"CM1\", \"CL1\", \"CR1\", \"EM1\", \"EL1\", \"ER1\",\n",
    "                                               \"CM2\", \"CL2\", \"CR2\", \"EM2\", \"EL2\", \"ER2\"])\n",
    "    avscore.to_csv(AV_OUT_FILE_NAME)\n",
    "    \n",
    "    # Saving single subject data for extinction-tonic pain comparisons\n",
    "    \n",
    "    extMiddle1 = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 2)]\n",
    "    Middle_ext_average1 = calculate_column_wise_average(extMiddle1)\n",
    "\n",
    "    extLeft1 = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 2)]\n",
    "    Left_ext_average1 = calculate_column_wise_average(extLeft1)\n",
    "\n",
    "    extRight1 = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 2)]\n",
    "    Right_ext_average1 = calculate_column_wise_average(extRight1)    \n",
    "    \n",
    "    extMiddle2 = newdf.loc[(newdf['start_new_trial_condition'] == 'MiddleLow') & (newdf['condorder'] == 4)]\n",
    "    Middle_ext_average2 = calculate_column_wise_average(extMiddle2)\n",
    "\n",
    "    extLeft2 = newdf.loc[(newdf['start_new_trial_condition'] == 'Left') & (newdf['condorder'] == 4)]\n",
    "    Left_ext_average2 = calculate_column_wise_average(extLeft2)\n",
    "\n",
    "    extRight2 = newdf.loc[(newdf['start_new_trial_condition'] == 'Right') & (newdf['condorder'] == 4)]\n",
    "    Right_ext_average2 = calculate_column_wise_average(extRight2)    \n",
    "    \n",
    "    frames = [Middle_ext_average1, Left_ext_average1, Right_ext_average1, Middle_ext_average2, Left_ext_average2, Right_ext_average2]\n",
    "    avscore2 = pd.concat(frames, axis=1, keys = [\"EM1\", \"EL1\", \"ER1\", \"EM2\", \"EL2\", \"ER2\"])\n",
    "    avscore2.to_csv(EXT_AV_OUT_FILE_NAME)\n",
    "    \n",
    "    # Read the CSV file for the current participant\n",
    "    participant_data = pd.read_csv(AV_OUT_FILE_NAME)\n",
    "    participant_data[\"Participant\"] = participantID\n",
    "    all_participants_data = pd.concat([all_participants_data, participant_data], ignore_index=True)\n",
    "    \n",
    "    ext_participant_data = pd.read_csv(EXT_AV_OUT_FILE_NAME)\n",
    "    ext_participant_data[\"Participant\"] = participantID\n",
    "    all_ext_participants_data = pd.concat([all_ext_participants_data, ext_participant_data], ignore_index=True)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb7d1cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/dhewitt/Data/pps/P01/P01_Extracted_1504.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m9/8l65rzcd5x5903xt73lz9ln00000gn/T/ipykernel_18559/2531674407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparticipantID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparticipant_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# ... (The existing code for processing a single participant will go here)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprocess_participant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticipantID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Concatenate all participant data into a single DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m9/8l65rzcd5x5903xt73lz9ln00000gn/T/ipykernel_18559/62197106.py\u001b[0m in \u001b[0;36mprocess_participant\u001b[0;34m(participantID)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mEXT_AV_OUT_FILE_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"P\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparticipantID\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/P\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparticipantID\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_PupilDiameterProcessed_AvExt_1904.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdata2load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_FILE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdataframe_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPKL_FILE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/dhewitt/Data/pps/P01/P01_Extracted_1504.csv'"
     ]
    }
   ],
   "source": [
    "# List of participant IDs\n",
    "#participant_ids = [\"P02\", \"P03\", \"P04\", \"P05\", \"P06\", \"P08\", \"P09\", \"P10\", \"P11\", \"P12\", \"P13\", \"P14\", \"P15\", \"P16\", \"P17\", \"P19\", \"P20\", \"P21\", \"P22\", \"P24\", \"P25\", \"P26\", \"P27\", \"P28\", \"P29\",\"P30\"]  # Add more participant IDs as needed\n",
    "#participant_ids = [\"02\", \"03\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"19\", \"20\", \"21\", \"22\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\",\"30\"] \n",
    "participant_ids = ['01']\n",
    "\n",
    "# Loop over each participant\n",
    "for participantID in participant_ids:\n",
    "    # ... (The existing code for processing a single participant will go here)\n",
    "    process_participant(participantID)\n",
    "    \n",
    "# Concatenate all participant data into a single DataFrame\n",
    "all_participants_data.to_csv(\"/Users/dhewitt/Data/pps/Exports/Combined_PupilDiameter_noav_0912.csv\", index=False)\n",
    "all_ext_participants_data.to_csv(\"/Users/dhewitt/Data/pps/Exports/Combined_PupilDiameterExt_0912.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958ebcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
